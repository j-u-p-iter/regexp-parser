# Tokenizer

## What is tokenizer?

A tokenizer transforms a sequence of characters into a sequence of tokens.

Tokenizers are also known as scanners or lexers. Tokenizers play a role in parsing, because they transform the initial input into a form that is more manageable by the proper parser, which works at a later stage. Typically tokenizers are easier to write than parsers. Although there are special cases when both are quite complicated, for instance in the case of C.
